{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Autoencoders\n",
    "\n",
    "There was one slide from Mike's lecture about how weights evolve\n",
    "Encodes and decodes input from same thing\n",
    "Unsupervised ML task (no label) - \n",
    "\n",
    "What we care about is output of hidden layer = compressed data = embedding = latent space\n",
    "Same thing as training any other neural net \n",
    "Non-linear dimensionality reduction (with sigmoid/ReLU activation functions) - not like PCA or others\n",
    "Can ignore decoder part and take embedding and do linear reduction - because they are \"better\" features\n",
    "\n",
    "The decoder is an example of a generative model - because they are dimensionally increasing\n",
    "\n",
    "* The purpose of this autoencoder is to produce a \"binding\" score between 0 and 1 for prob of being TF binding site\n",
    "* Note that training data is heavily weighted towards negative data\n",
    "* Should downsample neg data or duplicate positive  \n",
    "\n",
    "  \n",
    "* Part 2: Adaptation of code but not necessarily\n",
    "* Part 3: Cross evaluation - full credit should be from scratch - can use scikit learn but will be 1-2 points\n",
    "* Part 4: Attempt is good - just do hyperparameter optimization + try something cool - explain what you tried, how it did, and why you chose that\n",
    "* Part 5: Evaluate final model and see if your NN is working well (blinded on our side, TAs have labels)\n",
    "\n",
    "  \n",
    "* can create NN class, tell it dimension of output layer, other factors, etc\n",
    "* for encoding DNA data, one-hot encoding and flatten (eg [1,0,0,0] for A, [0,0,0,1] for G)\n",
    "\n",
    "* Why do you need a cost function that is readily differentiable? To get the simplest gradient possible since it is repeatedly calcualted during training\n",
    "* ie. cost function is how close you are to the target, the gradient is how fast you're approaching the target\n",
    "\n",
    "Training Goal: optimize J(theta) over choices of theta\n",
    "* Initialize weights (theta = t0, t1, ... ) = vector of small random values\n",
    "* Define a cost function J(theta)\n",
    "* Define a gradient dtheta(J(theta))\n",
    "* For t=1 ... max time --- set theta to (theta - alpha(dtheta)) --- what this is doing is moving the weights a little\n",
    "* Halt on convergence (when J(theta) doesn't change much anymore) or time out and report theta\n",
    "\n",
    "* Can perform sensitivity analysis on how changing parameters / training set changes ROC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61924043, 0.03240046, 0.12537537, 0.05353615, 0.02873017,\n",
       "        0.10444717, 0.00878742, 0.17182233],\n",
       "       [0.01783419, 0.29022285, 0.0534807 , 0.11589074, 0.01506372,\n",
       "        0.13276782, 0.29178005, 0.11815918],\n",
       "       [0.10646459, 0.04166288, 0.18054344, 0.1565528 , 0.21384123,\n",
       "        0.0502957 , 0.03777032, 0.04040759],\n",
       "       [0.02720728, 0.07754488, 0.14678251, 0.20005386, 0.18649696,\n",
       "        0.04898576, 0.10162535, 0.03090916],\n",
       "       [0.04330614, 0.03138022, 0.25370597, 0.25858886, 0.54762191,\n",
       "        0.0282026 , 0.04802834, 0.0150942 ],\n",
       "       [0.11606019, 0.16307883, 0.06781575, 0.07635392, 0.01616379,\n",
       "        0.15268816, 0.10203679, 0.1867526 ],\n",
       "       [0.0076296 , 0.30358028, 0.06227495, 0.16434759, 0.03252095,\n",
       "        0.10398725, 0.39941341, 0.07176241],\n",
       "       [0.19381545, 0.15543503, 0.06170166, 0.05902934, 0.00971223,\n",
       "        0.17823356, 0.0747741 , 0.24993524]])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 7)\n",
      "(3, 7)\n",
      "(8, 7)\n",
      "\n",
      "(3, 8)\n",
      "(8, 3)\n",
      "\n",
      "(8, 7)\n",
      "(3, 7)\n",
      "(8, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for l in n.layers:\n",
    "    print(l.shape)\n",
    "    \n",
    "print()\n",
    "    \n",
    "for l in n.weights:\n",
    "    print(l.shape)\n",
    "\n",
    "print()\n",
    "    \n",
    "for l in n.z_values:\n",
    "    print(l.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. let C be the error / loss (eg. calculated from BCE)\n",
    "\n",
    "2. let a(L) = activation of L = sigmoid(z(L)) where z(L) = w(L)a(L-1) + b(L)\n",
    "\n",
    "3. let dC/dw(L) be how sensitive the cost function is to w(L)\n",
    "- since the cost function is dependent on a(L) which is dependent on z(L), dC/dw(L) = dz(L)/dw(L) * da(L)/dz(L) * dC/da(L) by the chain rule\n",
    "- where dz(L)/dw(L) = dx activation* = a(L-1)\n",
    "- where da(L)/dz(L) = dx sigmoid function = d_sigmoid(Z(L))\n",
    "- where dC/da(L) = dx cost function = d_BCE(a(L))\n",
    "\n",
    "\\* Since Z(L) = w(L) * a(L-1) + b(L)\n",
    "\n",
    "4. so dC/dw(L) = a(L-1) * d_sigmoid(Z(L)) * d_BCE(a(L)) \n",
    "\n",
    "5. dC/da(L-1) = dz(L)\n",
    " \n",
    "6. and dC/db = dz(L)/db(L) * da(L)/dz(L) * dC/da(L)\n",
    "- where dz(L)/db(L) = 1\n",
    "- where da(L)/dz(L) = dx sigmoid function = d_sigmoid(Z(L))\n",
    "- where dC/da(L) = dx cost function = d_BCE(a(L))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# δL=gradient(a of C)∘sigmoid'(z(L))\n",
    "# δi=W(i+1).transpose() * δ(i+1) ∘sigmoid'(z(i))\n",
    "\n",
    "# dC/db(l,j) = δ(i,j)\n",
    "# dC/dw(l,j,i) = a(i-1, k) δ(i,j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: for batch gradient descent, shuffle training examples and divide into batches for backprop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-15-e2717e85c2bc>, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-e2717e85c2bc>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    pass\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "        # start with last layer and get delta L\n",
    "        # x(L) are the outputs (= the x values of the last layer)\n",
    "        # f'L is the derivative of the activation function for that layer\n",
    "        # δL=(x(L)−t)∘f′L(W(L)x(L−1))\n",
    "        \n",
    "        # for all the other layers 1 ... i , get the delta for i\n",
    "        # f'i is the derivative of the activaiton function in that layer (sigmoid) \n",
    "        # with respect to the current weights (between current layer and prev layer)\n",
    "        # W are the weights at the position i or i+1\n",
    "        # x are the values at the current layer\n",
    "        # Yes that is a recursive delta(i+1) being added so make sure to keep track of all the deltas\n",
    "        # δi = W(Transpose)(i+1) * δ(i+1) ∘ f′i(W(i) * x(i−1))\n",
    "        \n",
    "        # compute gradient of the current layer\n",
    "        curr_slope = self._dxSigmoid(self.curr_layer) # <- derivative of the activation function\n",
    "        \n",
    "        # compute weight modifications for output layer\n",
    "        delta_out = error * slope_out\n",
    "\n",
    "        # compute weight modification for hidden layer\n",
    "        # error of hidden layer is back-propagated from error at output\n",
    "        delta_hidden = np.dot(delta_out,self.wo.T) * slope_hidden\n",
    "        \n",
    "        # NOTE: out_gradient = 2*(y-pred)*sigmoid_derivative(self.output)\n",
    "        # what you're calculating is previous activaiton * loss\n",
    "\n",
    "        #θTx = θ0+nj = 1 θjxj\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        # np.dot(layer, delta) = gradient\n",
    "        # update the bias using the sum of the deltas\n",
    "        prev_layer = self.out_layer\n",
    "        for ind in range(self.hid_layers): # TODO: may be off by one, need to keep one for the input layer\n",
    "            self.weights[ind] += alpha * np.dot(prev_layer, curr_delta)\n",
    "            self.bias[ind] += self.delta[ind] * lr\n",
    "            prev_layer = self.hid_layers[ind] # update recursively to backpropagate through the network\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def gradientDescent():\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TODO: REMEMBER TO REPORT AVERAGE LOSS PER EPOCH\n",
    "    def lossDerivative(y, pred):\n",
    "        \"\"\"\n",
    "        Calculates the loss at each iteration using the binary cross-entropy (BCE) loss function.\n",
    "        \n",
    "        Loss appraoches infinity when (y - pred) -> 1, where y is the known class and pred is the \n",
    "        class predicted in the current iteration.\n",
    "        \n",
    "        Take derivative of \n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        y : int\n",
    "        Actual\n",
    "        pred : int\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # This is calculating the gradient, which is the derivative of the loss function\n",
    "        # When y = 0, returns 1 / (1-pred)\n",
    "        # When y = 1, returns -1 / pred\n",
    "        return (-y/pred) + ((1-y)/(1-pred)) \n",
    "    \n",
    "    def fit(self, x_train, y_train, activation):\n",
    "        \n",
    "        # ??? Do you shuffle the data when running multiple epochs\n",
    "        for epoch in self.epochs:\n",
    "            # take some subset for of values and outputs\n",
    "            for value, output in zip(x_train, y_train):\n",
    "                self.forward(value, activation)\n",
    "                #self.lossCalculation() ??? Do I need a separate function for this?\n",
    "                self.backpropagate(value, output)\n",
    "    \n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "# https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    print(activation)\n",
    "    \n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    \n",
    "    return activation\n",
    "\n",
    "def dotProduct(weights, inputs):\n",
    "    return np.dot(weights, inputs)\n",
    "    \n",
    "# Transfer neuron activation by sigmoid function\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "    \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(input_layer, network):\n",
    "    \n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        \n",
    "        #for neuron in layer:\n",
    "        \n",
    "        activation = activate(layer, input_layer)\n",
    "        \n",
    "        return activation\n",
    "    \n",
    "        neuron['output'] = transfer(activation)\n",
    "        new_inputs.append(neuron['output'])\n",
    "            \n",
    "    inputs = new_inputs\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n",
      "[0.6629970129852887, 0.7253160725279748]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    \n",
    "    # for each layer, update its weights\n",
    "    for layer in network:\n",
    "        layer = new_inputs.append(transfer(activate(layer, inputs)))\n",
    "\n",
    "# test forward propagation\n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "    \n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)\n",
    "print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot([1,2,3], [[1], [2], [3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeFasta(value):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
